---
layout: post
title: "sparkç¡¬æ ¸ä¼˜åŒ–2 limitä¼˜åŒ–"
date: 2023-11-06 23:23:23
categories: bigdata
tags: bigdata spark
keywords: Sparkæ‰§è¡Œä¼˜åŒ– spark limit
excerpt: æ·±å…¥åˆ†æå’Œè§£å†³limité€Ÿåº¦è¿‡æ…¢çš„é—®é¢˜, spark limitæ‰§è¡ŒåŸç†
comments: true
---
æ·±å…¥åˆ†æå’Œè§£å†³å¤§é‡çº§çš„limité€Ÿåº¦è¿‡æ…¢çš„é—®é¢˜, limitæ‰§è¡ŒåŸç†.

è¿™é‡Œæ˜¯å›å¿†æ•´ç†äº†ä¹‹å‰çš„ä¸¤ä¸ªcaseå†™æˆåšå®¢, åº”è¯¥æ˜¯æœ€åä¸¤ç¯‡å…³äºsparkçš„åšå®¢äº†.

åˆ†äº«è®°å½•å‡ ä¸ªåœ¨å®é™…å·¥ä½œä¸­è§£å†³çš„å‡ ä¸ªç¡¬æ ¸sparkä¼˜åŒ–çš„case, æ‰€è°“ç¡¬æ ¸å°±æ˜¯ä¸æ˜¯ç®€å•çš„æ”¹æ”¹sql/è°ƒè°ƒé…ç½®å°±èƒ½è§£å†³çš„, éœ€è¦æ·±å…¥sparkå†…éƒ¨åŸç†, ä¿®æ”¹/æ‰©å±•sparkæºç æ‰èƒ½å®ç°çš„ä¼˜åŒ–.

é•¿æ–‡é¢„è­¦, æ²¡æƒ³åˆ°è¿™ä¸ªlimitå†™äº†è¿™ä¹ˆå¤š.

* TOC
{:toc}

# limité€Ÿåº¦ä¸ºä»€ä¹ˆè¿™ä¹ˆæ…¢?
## é—®é¢˜
limitè¯­å¥åœ¨æ—¥å¸¸è·‘æ•°çš„æ—¶å€™æˆ‘ä»¬éƒ½ç»å¸¸ä½¿ç”¨, æ¯”å¦‚æƒ³çœ‹ä¸€ä¸‹æ•°æ®çš„æ ·å­, ä¸€èˆ¬éƒ½ä¼šæ‰§è¡Œ

{% highlight sql %}
select * from x limit 100;
{% endhighlight %}

è¿™ä¸ªè¯­å¥çš„æ‰§è¡Œé€Ÿåº¦ä¸€èˆ¬éƒ½éå¸¸å¿«, ç§’çº§å°±çœ‹åˆ°æ•°æ®å±•ç¤ºå‡ºæ¥äº†.

ä½†æ˜¯æœ‰æ—¶å€™æœ‰ä¸€äº›åœºæ™¯ä¸­, æˆ‘ä»¬éœ€è¦limitçš„é‡çº§å¤ªå¤§, ç”¨limitå°±ä¼šå‡ºå¥‡çš„æ…¢, æ¯”å¦‚ä»ä¸€ä¸ª100äº¿çš„è¡¨ä¸­`limit 1äº¿`æˆ–è€…ä»1äº¿1åƒä¸‡çš„è¡¨ä¸­`limit 1äº¿`éƒ½æ¯”é¢„æƒ³çš„æ…¢çš„å¤š, æ‰§è¡Œæ—¶é—´å°æ—¶çº§äº†.

## åŸå› 
æˆ‘æœ€åˆå…³äºlimitçš„æ‰§è¡Œè¿‡ç¨‹çš„è®¾æƒ³æ˜¯è¿™æ ·çš„:
- ç›´æ¥æ‹¿ç¬¬ä¸€ä¸ªpartitionæŒ‰è¡Œè¯», è¯»å¤Ÿäº†limitæ•°é‡ç›´æ¥è¿”å›. 
- å³ä½¿ç¬¬ä¸€ä¸ªpartitionå†…çš„æ•°é‡ä¸å¤Ÿ, å†æ‰“å¼€ç¬¬äºŒä¸ªpartitionç»§ç»­è¯». 
- å°±ç®—`limit-1äº¿`, æ•´ä¸ªè¿‡ç¨‹éƒ½é€€åŒ–æˆå•çº¿ç¨‹çš„è¿‡ç¨‹, é¡¶å¤šæ˜¯å•æœºread 1äº¿è¡Œçš„æ—¶é—´, åº”è¯¥ä¹Ÿä¸ä¼šæ…¢åˆ°é‚£ç§ç¨‹åº¦å•Š?

ä½†å®é™…sparkæ‰§è¡Œåº”è¯¥ä¸æ˜¯æˆ‘æƒ³è±¡çš„é‚£æ ·. åˆ†æä¸ºä»€ä¹ˆæ‰§è¡Œæ…¢, å…ˆæŸ¥çœ‹å¯¹åº”ä»»åŠ¡sparké¡µé¢, å‘ç°åœ¨æŸä¸ªstageä¸Šå¼€å§‹å•taskæ‰§è¡Œäº†, å¡ç‚¹å¯èƒ½åœ¨è¿™é‡Œ.

åœ¨å…·ä½“åˆ†æå…¶å†…éƒ¨æ‰§è¡ŒåŸç†å‰, æˆ‘å¿½ç„¶å‘ç°å‡ ç§limitçš„åœºæ™¯è¿˜æœ‰äº›ä¸åŒ, è¿™é‡Œå±•ç¤ºäº†ä¸€ä¸‹.
### å‡ ç§limitä½¿ç”¨åœºæ™¯
#### æƒ…å†µä¸€(Query 5), limitä¹‹åé‡‡é›†åˆ°driverç«¯ä½œä¸ºç»“æœ(actionç®—å­)
{:.no_toc}
```
val df = spark.table("db1.table1")
df.count() //57 0711 4895
df.show(1000000)
== Physical Plan ==
CollectLimit (4)
+- * Project (3)
   +- * ColumnarToRow (2)
      +- Scan orc db1.table1 (1)
```
#### æƒ…å†µäºŒ(Query 6), limitä¹‹åå†™å‡ºè¡¨/æ–‡ä»¶
{:.no_toc}
```
val df1 = spark.sql("select * from df limit 100000")
df1.write.csv("/user/tianzhipeng-jk/temp1107")
== Physical Plan ==
Execute InsertIntoHadoopFsRelationCommand (11)
+- AdaptiveSparkPlan (10)
   +- == Current Plan ==
      GlobalLimit (6)
      +- ShuffleQueryStage (5)
         +- Exchange (4)
            +- * LocalLimit (3)
               +- * ColumnarToRow (2)
                  +- Scan orc db1.table1 (1)
   +- == Initial Plan ==
      GlobalLimit (9)
      +- Exchange (8)
         +- LocalLimit (7)
            +- Scan orc db1.table1 (1)
```
#### æƒ…å†µä¸‰(Query 9), limitä¹‹åç»§ç»­å‚ä¸åç»­çš„å˜æ¢è¿ç®—(limitä¹‹åä½œä¸ºdf)
{:.no_toc}
```
val other = spark.table("db1.table2")
other.createOrReplaceTempView("other")
val df2 = spark.sql("select * from other a left join (select * from df limit 1000000) b on a.deviceid=b.deviceid")
df2.show(100000)
== Physical Plan ==
AdaptiveSparkPlan (34)
+- == Final Plan ==
   CollectLimit (21)
   +- * Project (20)
      +- * SortMergeJoin LeftOuter (19)
         :- * Sort (7)
         :  +- AQEShuffleRead (6)
         :     +- ShuffleQueryStage (5), Statistics(sizeInBytes=88.7 MiB, rowCount=1.66E+6)
         :        +- Exchange (4)
         :           +- * LocalLimit (3)
         :              +- * ColumnarToRow (2)
         :                 +- Scan orc db1.table2 (1)
         +- * Sort (18)
            +- AQEShuffleRead (17)
               +- ShuffleQueryStage (16), Statistics(sizeInBytes=53.4 MiB, rowCount=1.00E+6)
                  +- Exchange (15)
                     +- * Filter (14)
                        +- * GlobalLimit (13)
                           +- ShuffleQueryStage (12), Statistics(sizeInBytes=106.8 GiB, rowCount=2.05E+9)
                              +- Exchange (11)
                                 +- * LocalLimit (10)
                                    +- * ColumnarToRow (9)
                                       +- Scan orc db1.table1 (8)
```

å¯ä»¥çœ‹å‡º, ä¸»è¦åŒºåˆ«è¿˜æ˜¯limitä¹‹åæ˜¯ä½œä¸ºç»“æœæ”¶é›†åˆ°driverè¿˜æ˜¯ä½œä¸ºdfå‚ä¸åç»­è®¡ç®—, ä»é‡Œé¢å¯ä»¥çœ‹å‡ºä¸åŒåœºæ™¯å¯¹åº”çš„ç‰©ç†æ‰§è¡Œè®¡åˆ’ç®—å­æ˜¯ä¸åŒçš„

### limitå¯¹åº”çš„ç‰©ç†æ‰§è¡Œè®¡åˆ’ç®—å­
ç»“åˆä¸Šé¢æ‰§è¡Œè®¡åˆ’ä¸­çš„ç®—å­æœç´¢äº†ä¸€ç•ª, å‘ç°å…¶æºç éƒ½ä½äº`org/apache/spark/sql/execution/limit.scala`, è¿™ä¸ªæºæ–‡ä»¶ä¸­åŒ…å«æ‰€æœ‰limitç›¸å…³çš„ç‰©ç†æ‰§è¡Œè®¡åˆ’ç®—å­.

- CollectLimitExec: 
    
    Take the first limit elements and collect them to a single partition. This operator will be used when a logical Limit operation is the final operator in an logical plan, which happens when the user is collecting results back to the driver.

    å–å‰é¢çš„è‹¥å¹²ä¸ªé™åˆ¶å…ƒç´ ï¼Œå¹¶å°†å®ƒä»¬æ”¶é›†åˆ°ä¸€ä¸ªå•ç‹¬çš„åˆ†åŒºã€‚è¿™ä¸ªè¿ç®—ç¬¦å°†ç”¨äºé€»è¾‘è®¡åˆ’ä¸­çš„æœ€åä¸€ä¸ªé€»è¾‘é™åˆ¶æ“ä½œï¼Œå½“ç”¨æˆ·å°†ç»“æœæ”¶é›†å›é©±åŠ¨ç¨‹åºæ—¶ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚
- CollectTailExec:
    
    Take the last limit elements and collect them to a single partition.This operator will be used when a logical Tail operation is the final operator in an logical plan, which happens when the user is collecting results back to the driver.
    
    å–æœ€åçš„è‹¥å¹²ä¸ªé™åˆ¶å…ƒç´ ï¼Œå¹¶å°†å®ƒä»¬æ”¶é›†åˆ°ä¸€ä¸ªå•ç‹¬çš„åˆ†åŒºã€‚è¿™ä¸ªè¿ç®—ç¬¦å°†ç”¨äºé€»è¾‘è®¡åˆ’ä¸­çš„æœ€åä¸€ä¸ªé€»è¾‘å°¾éƒ¨æ“ä½œï¼Œå½“ç”¨æˆ·å°†ç»“æœæ”¶é›†å›é©±åŠ¨ç¨‹åºæ—¶ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚
- BaseLimitExec: ä¸‹è¿°LocalLimitExec/GlobalLimitExecç®—å­çš„åŸºç±».
- LocalLimitExec: 
    
    Take the first limit elements of each child partition, but do not collect or shuffle them.
    
    å–æ¯ä¸ªå­åˆ†åŒºçš„å‰è‹¥å¹²ä¸ªé™åˆ¶å…ƒç´ ï¼Œä½†ä¸è¿›è¡Œæ”¶é›†æˆ–æ´—ç‰Œã€‚
- GlobalLimitExec:

    Take the first limit elements of the child's single output partition.

    å–å­èŠ‚ç‚¹çš„å•ä¸ªè¾“å‡ºåˆ†åŒºçš„å‰è‹¥å¹²ä¸ªé™åˆ¶å…ƒç´ ã€‚

- TakeOrderedAndProjectExec:

    Take the first limit elements as defined by the sortOrder, and do projection if needed. This is logically equivalent to having a Limit operator after a SortExec operator, or having a ProjectExec operator between them. This could have been named TopK, but Spark's top operator does the opposite in ordering so we name it TakeOrdered to avoid confusion.

    val df3 = spark.sql("select * from other a left join (select * from df order by score limit 1000000) b on a.deviceid=b.deviceid")

è¿™äº›ç‰©ç†ç®—å­, åœ¨SparkPlannerçš„ç­–ç•¥ä¸­è¢«é…ç½®:
- SpecialLimitsç­–ç•¥ä¸­, æ ¹æ®ä¸€äº›æ¡ä»¶, å°†é€»è¾‘è®¡åˆ’è½¬ä¸ºCollectLimitExecæˆ–TakeOrderedAndProjectExecç­‰ç‰©ç†ç®—å­
- BasicOperatorsç­–ç•¥ä¸­, åœ¨ä¸æ»¡è¶³ä¸Šè¿°æ¡ä»¶æƒ…å†µä¸‹çš„limité€»è¾‘è®¡åˆ’, è½¬ä¸ºLocalLimitExecå’ŒGlobalLimitExecç‰©ç†ç®—å­

æ ¹æ®ä¸Šé¢åœºæ™¯ä¸¾ä¾‹, æˆ‘ä»¬åªåˆ†æè¿™3ä¸ªExecçš„åŸç†
- ç»“æœæ”¶é›†åˆ°driver: CollectLimitExec
- ä½œä¸ºdfå‚ä¸åç»­è®¡ç®—: LocalLimitExecå’ŒGlobalLimitExecé…åˆ

### CollectLimitExec/LocalLimitExec/GlobalLimitExecåŸç†

#### CollectLimitExecçš„doExecuteæ–¹æ³•
{:.no_toc}

{% highlight scala %}
    val locallyLimited = childRDD.mapPartitionsInternal(_.take(limit))
    new ShuffledRowRDD(
        ShuffleExchangeExec.prepareShuffleDependency(
        locallyLimited,
        child.output,
        SinglePartition,
        serializer,
        writeMetrics),
        readMetrics)
    }
    singlePartitionRDD.mapPartitionsInternal(_.take(limit))
{% endhighlight %}

åˆ†ä¸ºä¸‰æ­¥:
1. å¯¹äºå­RDD(ä¸Šæ¸¸RDD), ä½¿ç”¨mapPartitionsInternal, å¯¹æ¯ä¸ªpartitionsæ‰§è¡Œå–å‰1äº¿è¡Œçš„æ“ä½œ
2. å°†ç¬¬ä¸€æ­¥è¾“å‡ºRDDè¿›è¡Œshuffle, æ··æ´—æˆä¸€ä¸ªå•åˆ†åŒºRDD(SinglePartition)
3. å¯¹å•åˆ†åŒºRDDå†åšä¸€æ¬¡å–å‰1äº¿è¡Œçš„æ“ä½œ

å¥½å®¶ä¼™, ç ´æ¡ˆäº†, è¿™ä¸ªlimitè¦æ‰§è¡Œå¤šå°‘æ¬¡`æ•°1äº¿è¡Œ`çš„æ“ä½œå•Š, ç¡®å®ä¼šæ¯”å‰æ–‡æˆ‘è®¾æƒ³çš„å•æœºè¯»1ä¸€è¡Œæ…¢å¤šäº†:

å‡ä½¿100ä¸ªåˆ†åŒº, ç›¸å½“äºå…ˆå¹¶è¡Œçš„è¿›è¡Œäº†100æ¬¡`æ•°1äº¿è¡Œ`çš„æ“ä½œ, é‡ç‚¹æ˜¯è¦å°†è¿™äº›100*1äº¿çš„æ•°æ®shuffleåˆ°ä¸€ä¸ªåˆ†åŒº, å†åœ¨è¿™ä¸ªåˆ†åŒºä¸Šæ‰§è¡Œ1æ¬¡`æ•°1äº¿è¡Œ`çš„æ“ä½œ, å…‰shuffleåˆ°ä¸€ä¸ªåˆ†åŒºè¿™ä¸€æ­¥å°±æ˜¯ä¸å¯æ¥å—çš„äº†.

è¿˜å¥½çš„ä¸€ç‚¹æ˜¯, æ²¡äººä¼šæƒ³è¦`limit-1äº¿`ä¹‹å, æŠŠ1äº¿è¡Œshowå±•ç¤ºåœ¨consoleä¸Š, å¦åˆ™è¿™è‚¯å®šæ‰§è¡Œä¸å‡ºæ¥å•Š?

#### å†çœ‹ä¸€ä¸‹LocalLimitExec/GlobalLimitExec
{:.no_toc}

è¿™ä¸¤ä¸ªç±»é‡Œé¢åŸºæœ¬éƒ½æ²¡æœ‰ä»£ç , å…¶doExecuteæ–¹æ³•ç»§æ‰¿è‡ªåŸºç±»:
```
  protected override def doExecute(): RDD[InternalRow] = child.execute().mapPartitions { 
    iter => iter.take(limit)
  }
```

é›¾è‰, è¿™ä¸ªè¿˜ä¸Šè¿°CollectLimitExecå¦‚å‡ºä¸€è¾™å•Š, ä¹Ÿå°±æ˜¯LocalLimitåœ¨æ¯ä¸ªåˆ†åŒºå–1äº¿. GlobalLimitæœ€ç»ˆå†å–ä¸€äº¿, ä¸­é—´å€ŸåŠ©spark-sqlçš„Exchangeç®—å­åšshuffle, åœ¨GlobalLimitå®šä¹‰äº†

```
override def requiredChildDistribution: List[Distribution] = AllTuples :: Nil
```
AllTuplesä¹Ÿæ˜¯å•åˆ†åŒºçš„å•Š, é€»è¾‘å‡ ä¹ä¸€æ¨¡ä¸€æ ·, çœŸå‘å•Š

<!-- TODO limit1äº¿çœŸæ‰§è¡Œä»¥ä¸‹. ç¡®è®¤æ˜¯å¦æ˜¯shuffleåˆ°ä¸€ä¸ªåˆ†åŒº, å¹¶è®°å½•å®é™…æ‰§è¡Œæ—¶é—´ nohup python x.py 2>&1 -->

### CollectLimitExecçœŸå®åŸç†å’ŒlimitScaleUpFactorå¢é‡å¼limit
åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­, å‘ç°ä¸€ä¸ªsparkä¸­å…³äºlimitçš„å‚æ•°`spark.sql.limit.scaleUpFactor`, åœ¨ä»”ç»†ç ”ç©¶åå‘ç°, åˆšæ‰çš„åˆ†ææœ‰ä¸€ä¸ªåœ°æ–¹å‡ºé”™äº†!!

#### SparkPlanç±»çš„executeTake
{:.no_toc}
ä¸Šè¿°é…ç½®è¢«åŠ è½½åˆ°`SQLConf.limitScaleUpFactor`, å®ƒåªåœ¨ä¸€ä¸ªåœ°æ–¹è¢«è°ƒç”¨, å°±æ˜¯SparkPlanç±»çš„executeTakeæ–¹æ³•.

SparkPlanç±»å°±æ˜¯æˆ‘ä»¬æ‰€æœ‰ç‰©ç†æ‰§è¡Œç®—å­çš„åŸºç±», æˆ‘ä¸€èˆ¬éƒ½åªå…³æ³¨å…¶doExecuteæ–¹æ³•, å­ç±»å®ç°çš„æ—¶å€™ä¸€èˆ¬ä¹Ÿåªè¦†ç›–doExecuteæ–¹æ³•, é‚£è¿™ä¸ªexecuteTakeæ˜¯å¹²å˜›ç”¨çš„å‘¢?

executeTakeæ–¹æ³•æ³¨é‡Šä¸­å†™é“`Runs this query returning the first n rows as an array.` çœ‹èµ·æ¥å°±æ˜¯è·å–dataframeå‰å‡ è¡Œæ•°æ®ç”¨çš„, å’Œlimitå‡ ä¹å¾ˆåƒ, å½“ç„¶executeTakeæ˜¯æŠŠè¿™å‡ è¡Œæ•°æ®ä½œä¸ºæ•°ç»„è¿”å›çš„, æ˜¯driverç«¯ä½¿ç”¨çš„. 

å…¶æ ¸å¿ƒä»£ç é€»è¾‘å¦‚ä¸‹:

{% highlight scala %}
val childRDD = getByteArrayRdd(n)
val buf =  new ArrayBuffer[InternalRow]
val totalParts = childRDD.partitions.length
var partsScanned = 0
while (buf.length < n && partsScanned < totalParts) {
  //-- â‘  -- 
  var numPartsToTry = 1L // æœ¬è½®è¦å°è¯•æ‰«æçš„partitionæ•°é‡, åˆå§‹æ˜¯1.
  if (partsScanned > 0) { // å·²ç»æ‰«è¿‡äº†ä¸€äº›partition, ä½†æ˜¯è¿˜æ²¡å‡‘å¤Ÿnè¡Œæ•°æ®, é‚£ä¹ˆè¿™è½®æ‰«æçš„partitionæ•°é‡è¦åŠ å€
    val limitScaleUpFactor = Math.max(conf.limitScaleUpFactor, 2)
    if (buf.isEmpty) {
      numPartsToTry = partsScanned * limitScaleUpFactor
    } else {
      val left = n - buf.length
      numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.length).toInt
      numPartsToTry = Math.min(numPartsToTry, partsScanned * limitScaleUpFactor) //æ ¹æ®ç¼ºå°‘çš„è¡Œæ•° å’Œ æ”¾å¤§ç³»æ•°å…±åŒå†³å®šä¸€ä¸ªæœ¬æ¬¡partæ•°
    }
  }
  
  //-- â‘¡ -- 
  val parts = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt)
  val sc = sparkContext
  //è¿™é‡Œæœ‰ä¸‰ä¸ªè¦ç‚¹
  //  1. runJobçš„æœ€åä¸€ä¸ªå‚æ•°å¯ä»¥æŒ‡å®šæœ¬ä¸ªjobåœ¨rddçš„å“ªäº›partitionæ˜¯æ‰§è¡Œ, å¯ä»¥ä¸åœ¨æ‰€æœ‰partä¸Šæ‰§è¡Œ
  //  2. runJobçš„ç¬¬äºŒä¸ªå‚æ•°`func: Iterator[T] => U`ç›´æ¥è¿”å›çš„it.next(), è¿™ä¸ªitæ˜¯ç¬¬ä¸€è¡ŒgetByteArrayRddå¤„ç†å¥½çš„æ•°æ®è¡Œ
  //  3. runJobçš„è¿”å›å€¼æ˜¯ä¸€ä¸ªå†…å­˜æ•°ç»„, driverç«¯çš„
  val res = sc.runJob(childRDD, (it: Iterator[(Long, Array[Byte])]) =>
    if (it.hasNext) it.next() else (0L, Array.emptyByteArray), partsToScan)

  //-- â‘¢ -- 
  var i = 0
  while (buf.length < n && i < res.length) {
    val rows = decodeUnsafeRows(res(i)._2)
    if (n - buf.length >= res(i)._1) {
      buf ++= rows.toArray[InternalRow]
    } else {
      buf ++= rows.take(n - buf.length).toArray[InternalRow]
    }
    i += 1
  }
  partsScanned += partsToScan.size
}
{% endhighlight %}

ä¸»ä½“æ˜¯ä¸€ä¸ªå¾ªç¯, æ¯è½®å¾ªç¯ä¼šè¯»å–ä¸€å®šé‡çš„partitioné‡Œçš„è¡Œ. å¾ªç¯å†…ä»£ç å¤§æ¦‚3æ­¥:
1. å†³å®šæœ¬è½®è¦å°è¯•æ‰«æçš„partitionæ•°é‡. æ˜¯æ ¹æ®`ç¼ºå°‘çš„è¡Œæ•°å’Œæ”¾å¤§ç³»æ•°`å…±åŒå†³å®šçš„. ç®€å•ç†è§£å¯ä»¥çœ‹åšç¬¬ä¸€æ¬¡æ‰«æ1ä¸ªpartiton, ç¬¬äºŒæ¬¡æ‰«æ2ä¸ªpartition, ç¬¬ä¸‰æ¬¡æ‰«æ4ä¸ªpartition(ä¸é‡å¤çš„)
2. ç¬¬äºŒæ­¥ä½¿ç”¨runJobå°†è¦æ‰«æçš„paritionçš„è¡Œè¯»å–åˆ°driver.
3. ç¬¬ä¸‰æ­¥å°†limitæ‰€éœ€æ•°é‡çš„è¡Œ, è§£ç æ”¾å…¥buffer

è¿™ç§å¢é‡å¼çš„limit, å’Œæˆ‘ä¹‹å‰æ„æƒ³çš„ç±»ä¼¼, ä¸éœ€è¦shuffle, è€Œä¸”æ¯”æˆ‘è®¾æƒ³çš„æ›´å¥½, æœ‰ä¸ªå¢é‡è®¾è®¡, è™½ç„¶ä¹Ÿåªèƒ½æ˜¯å•çº¿ç¨‹çš„. 

é‚£ä¹ˆè¿™ä¸ªexecuteTakeä½•æ—¶è¢«ä½¿ç”¨å‘¢? åœ¨limitä¸­ä¸æ¶‰åŠä¹ˆ? é€šè¿‡ideaæŸ¥çœ‹executeTakeæ–¹æ³•è¢«è°ƒç”¨çš„åœ°æ–¹:

<img src="/resources/sparklimit/1.png" width="700" alt="1"/>

å¯ä»¥çœ‹å‡ºæ˜¯åœ¨å¾ˆå¤šæ¡†æ¶å†…éƒ¨çš„å„ç±»å·¥å…·æ€§è´¨çš„åœ°æ–¹è¢«ä½¿ç”¨, å®ƒä»¬å¯èƒ½å¶å°”éœ€è¦ç”¨çš„rddçš„å‡ è¡Œæ•°æ®, åˆ™è°ƒç”¨è¿™ä¸ª. (eg. è¯»å–å¸¦headerçš„csvæ—¶)

ä½†æ˜¯, å¯ä»¥çœ‹å‡ºå›¾ä¸­æœ€åä¸€è¡Œ, å®ƒåœ¨limit.scalaä¸­è¢«ä½¿ç”¨äº†!!

#### CollectLimitExec.executeCollect
{:.no_toc}
è¿½è¸ªè¿‡å»å¯ä»¥çœ‹åˆ°åœ¨CollectLimitExecä¸­

`override def executeCollect(): Array[InternalRow] = child.executeTake(limit)`

å‘ƒå‘ƒ, åˆå¤šä¸ªexecuteCollectæ–¹æ³•, å®ƒè°ƒç”¨äº†ä¸Šè¿°çš„executeTake... ä¸æƒ³å†è¿½è¸ªä»£ç äº†, ä¸ºäº†æ¢ç©¶åœ¨å®é™…ä½¿ç”¨limitçš„æ—¶å€™æ˜¯ç”¨çš„CollectLimitExecçš„`doExecute`è¿˜æ˜¯`executeCollect`æ–¹æ³•, æˆ‘ç›´æ¥æœ¬æœºdebugåŠ æ–­ç‚¹æ‰§è¡Œäº†ä¸€ä¸‹:

1. æ— limitç›´æ¥show df
   
   <img src="/resources/sparklimit/æ— limitç›´æ¥show df.png" width="700" alt="1"/>

2. limitä¹‹åshow
   
   <img src="/resources/sparklimit/limitä¹‹åshow.png" width="700" alt="1"/>

å¯ä»¥çœ‹å‡º, ç¡®å®ä½¿ç”¨çš„æ˜¯executeTakeçš„å¢é‡å¼limité€»è¾‘!

ä½†æ˜¯è¿™ä¸ªé€»è¾‘ä»…ä»…åœ¨å¦‚ä¸‹æƒ…å†µä¸‹èƒ½ç”¨, å…¶ä»–çš„åœºæ™¯åƒæ˜¯limitä¹‹åä½œä¸ºdf, è‚¯å®šæ˜¯ç”¨ä¸ä¸Šäº†.
- æ¡†æ¶å†…éƒ¨å·¥å…·, å¶å°”ä¼šæ‰§è¡Œrdd.executeTakeè·å–å‡ è¡Œæ•°æ®.
- CollectLimitExecç®—å­, ä¹Ÿå°±æ˜¯limitä¹‹åcollectåˆ°driverç«¯çš„æƒ…å†µ.


<!-- - limitä¹‹åshowçš„, ç¡®å®šç”¨çš„æ˜¯è¿™ä¸ªå§? (çœ‹çœ‹æœ‰æ²¡æœ‰shuffleå°±çŸ¥é“äº†) æ˜¯ç”¨çš„è¿™ä¸ª. -->
<!-- - limitä¹‹åä½œä¸ºdfçš„, è‚¯å®šç”¨ä¸ä¸Šå§? ç›å¾·æ™ºéšœ, æ‰§è¡Œè®¡åˆ’ä¸­éƒ½æ²¡æœ‰CollectLimitExec, è‚¯å®šç”¨ä¸ä¸Šå•Š-->

## è§£æ³•
å…¶å®åˆ†æäº†ä¸€å¤§æ³¢limitä¸ºä»€ä¹ˆè¿™ä¹ˆæ…¢, å¯¹äºè§£å†³è¿™ä¸ªé—®é¢˜å¥½åƒå¸®åŠ©ä¸å¤§, å³ä½¿æ˜¯æ”¹è¿›çš„å¢é‡å¼limit, ä¾æ—§æ˜¯å•æœºlimit, é€Ÿåº¦è¿˜æ˜¯æ…¢. æ¥ä¸‹æ¥, æˆ‘æ ¹æ®éœ€æ±‚çš„3ç§ä¸åŒæƒ…å†µ, è®¾è®¡äº†å¯¹åº”çš„è§£å†³æ–¹æ¡ˆ.

ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ, éƒ½æ˜¯å¹¶è¡Œçš„, éƒ½ä¸æ¶‰åŠæ•°æ®shuffle, é€Ÿåº¦é‚£æ˜¯æ¯”åŸæ¥çš„å¿«çš„å¤š.

### ä¸ç²¾ç¡®limit-1äº¿æ–¹æ¡ˆ
æœ‰äº›æ—¶å€™å…¶å®åªæ˜¯éœ€è¦limitæ§åˆ¶ä¸€ä¸‹æ•°æ®é‡, ä¸æ˜¯ç²¾å‡†çš„1äº¿, å¤šå‡ ä¸‡å°‘å‡ ä¸‡ä¸ªä¹Ÿæ²¡å…³ç³»çš„è¯, å°±æœ‰ä¸€äº›ä¸ç²¾å‡†ä½†å¿«é€Ÿçš„è§£å†³åŠæ³•.

ä»¥ä¸‹æˆ‘å†™çš„ä¸‰ç§éƒ½æ˜¯åˆ©ç”¨éšæœºæ•°çš„æ–¹æ¡ˆ, é¦–å…ˆæˆ‘ä»¬è¦ç¡®å®š`æ€»æ•°æ®é‡çº§`å’Œ`è¦æŠ½å–çš„æ•°æ®é‡çº§`, æ¯”å¦‚åˆ†åˆ«æ˜¯100äº¿å’Œ1äº¿, é‚£ä¹ˆæˆ‘ä»¬è¦æŠ½å–çš„æ¯”ä¾‹å°±æ˜¯1/100.

1. **TABLESAMPLE**

   `SELECT * FROM test TABLESAMPLE (50 PERCENT)`
   
   æ ¹æ®[æ–‡æ¡£](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-sampling.html)TABLESAMPLEæœ‰ä¸‰ç§å‚æ•°å†™æ³•, ç™¾åˆ†æ¯”(ğŸ‘†), åˆ†æ•°(`TABLESAMPLE(BUCKET x OUT OF y)`)å’Œè¡Œæ•°`TABLESAMPLE(x ROWS)`.
   
   âš ï¸âš ï¸âš ï¸ ä¸è¦ä»¥ä¸ºç”¨è¿™ä¸ªè¡Œæ•°çš„å†™æ³•å°±èƒ½ç›´æ¥å®ç°å¿«é€Ÿçš„limit, å®ƒæœ¬è´¨ä¸Šè¿˜æ˜¯è°ƒç”¨ä¸Šè¿°sparkå†…éƒ¨çš„limitå®ç°çš„, é€Ÿåº¦è¿˜æ˜¯å¾ˆæ…¢. å‰ä¸¤ç§éšæœºæŠ½å–çš„æ˜¯çœŸå¹¶è¡Œçš„, å¾ˆå¿«.

   ```
   //org.apache.spark.sql.catalyst.plans.logical.Sample
   //org.apache.spark.sql.execution.SampleExec
   //org/apache/spark/spark-catalyst_2.13/3.2.0/spark-catalyst_2.13-3.2.0-sources.jar!/org/apache/spark/sql/catalyst/parser/AstBuilder.scala:1200
   ctx.sampleMethod() match {
      case ctx: SampleByRowsContext =>   // æ³¨æ„è¿™é‡Œ, å¦‚æœæ˜¯TABLESAMPLE(x ROWS), ç›´æ¥è§£ææˆLimité€»è¾‘èŠ‚ç‚¹äº†!!!
        Limit(expression(ctx.expression), query)

      case ctx: SampleByPercentileContext =>
        val fraction = ctx.percentage.getText.toDouble
        val sign = if (ctx.negativeSign == null) 1 else -1
        sample(sign * fraction / 100.0d)
   ```

   ä»ä¸€ä¸ª57äº¿çš„è¡¨éšæœºæŠ½çº¦1äº¿çš„æ‰§è¡Œæ—¶é—´: 2.3 min

   <!-- fin_dm_data_ai.dm_ads_prea_all_score_fusion_6_7 -->
   <!-- fin_dm_data_ai.dm_ads_fkblack_wj_all -->
   <!-- 5707114895 -->
   <!-- spark.sql("select * from df TABLESAMPLE (1 PERCENT)").write.csv("/user/tianzhipeng-jk/temp1109_2") -->
   <!-- æ—¶é—´ 2.3 min	-->

   <!-- val df500 = df.repartition(500) -->
   <!-- df500.write.csv("/user/tianzhipeng-jk/temp1109_df500") -->
   <!-- val df500 = spark.read.csv("/user/tianzhipeng-jk/temp1109_df500") -->
   <!-- df500.createOrReplaceTempView("df500") -->

   <!-- åšæŒè¦2000ä¸ªåˆ†åŒº... 5707114895/2000 = 2853557.4475 -->
   <!-- implicit val encoder = RowEncoder(df500.schema) -->
   <!-- df500.mapPartitions((a)=>a.take(2853557)).write.csv("/user/tianzhipeng-jk/temp1109_5") -->
   <!-- æ—¶é—´ 3.1 min	 -->
   <!-- spark.sql("select * from df500 TABLESAMPLE (BUCKET 1 OUT OF 57)").write.csv("/user/tianzhipeng-jk/temp1109_6") -->


2. **df.sample(0.01)å’Œrdd.sample(0.01)**
3. **rand**

   `select * from x where rand() < 0.01`


### ç²¾ç¡®limit-1äº¿æ–¹æ¡ˆ
å¦‚æœçœŸçš„ä¸€å®šè¦æ­£æ­£å¥½å¥½1äº¿æ¡æ•°æ®, é‚£ä¹ˆä¹Ÿæœ‰ä¸€äº›å¿«ä¸€ç‚¹çš„æ–¹æ¡ˆ

#### ç›´æ¥åˆ†åŒºtake
{:.no_toc}

å‡è®¾æˆ‘ä»¬çš„æ•°æ®å„åˆ†åŒºé‡çº§å‡åŒ€, é‚£ä¹ˆæˆ‘ä»¬åœ¨æ¯ä¸ªåˆ†åŒºå–ä¸€éƒ¨åˆ†, åˆèµ·æ¥ç­‰äºæ€»éœ€æ±‚é‡å°±å¥½äº†. æ¯”å¦‚æœ€ç»ˆè¦1äº¿, ä¸€å…±50ä¸ªåˆ†åŒº, é‚£ä¹ˆæ¯ä¸ªåˆ†åŒºå–200ä¸‡å°±å¯ä»¥.

```
implicit val encoder = RowEncoder(df.schema)
df.mapPartitions((a)=>a.take(2853557))
```

ä»ä¸€ä¸ª57äº¿çš„è¡¨éšæœºæŠ½çº¦1äº¿çš„æ‰§è¡Œæ—¶é—´: 3.1 min

å½“ç„¶è¿™ç§ç”¨æ³•å°±æ‹…å¿ƒåˆ†åŒºä¸å‡åŒ€, æŸä¸ªåˆ†åŒºä¸å¤Ÿ200ä¸‡, ä½†å®é™…å·¥ä½œä¸­å¾ˆå°‘é‡åˆ°, æ‰€ä»¥æ•ˆæœå®é™…ä¸é”™.

#### åˆ†åŒºcount/åˆ†åŒºtake
{:.no_toc}

å¦‚æœæ‹…å¿ƒä¸Šé¢å„åˆ†åŒºç›´æ¥take200ä¸‡ä¸å¤ªæ”¾å¿ƒ, å¯ä»¥ä¼˜åŒ–ä¸€ä¸‹, å…ˆç»Ÿè®¡å„åˆ†åŒºæ¡æ•°, åˆ†é…ä¸€ä¸‹æ€»å…±1äº¿åœ¨å„åˆ†åŒºå–å¤šå°‘, ç„¶åå†take

{% highlight scala %}
   //  â‘   ç»Ÿè®¡æ¯ä¸ªåˆ†åŒºå†…è¡Œæ•°
    val x = df5.mapPartitions((a) => {
      val pid = TaskContext.getPartitionId()
      Iterator((pid, a.size))
    })
    val countByPart = x.collectAsList()
    print(countByPart) //[(0,400), (1,400), (2,400), (3,400), (4,400)]
   //  â‘¡  åˆ†é…å„åˆ†åŒºåº”è¯¥takeçš„æ•°é‡. éšä¾¿å†™ä¸ªå‘†å‘†çš„ç®—æ³•.
    var limit = 900
    val takeByPart = new Array[Int](countByPart.size)
    for (a <- 0 until countByPart.size) {
      val take = if (limit > 0) {
        Math.min(limit, countByPart.get(a)._2)
      } else {
        0
      }
      limit = limit - take
      takeByPart(a) = take
    }
    print(takeByPart.mkString("(", ", ", ")")) //(400, 400, 100, 0, 0)
    val takeByPartBC = spark.sparkContext.broadcast(takeByPart)
   //  â‘¢  åˆ†åŒºtakeç»“æœ
    val result = df5.mapPartitions((a) => {
      val pid = TaskContext.getPartitionId()
      val take = takeByPartBC.value(pid)
      a.take(take)
    })
    assert(result.count() == 900)
{% endhighlight %}


### ç²¾ç¡®ä¸”çœŸéšæœº1äº¿æ–¹æ¡ˆ
å“ˆå“ˆ, å…¶å®è¿™é‡Œå°±æ˜¯ç©ä¸€ä¸‹è“„æ°´æ± æŠ½æ ·è€Œå·². å½“ç„¶å¯èƒ½ä¹Ÿæœ‰å®é™…åœºæ™¯éœ€æ±‚:

`å‡è®¾æˆ‘ä»¬è¦ä»100äº¿ä¸­ç²¾ç¡®å–1äº¿, ä¸”ä¿è¯æ¯æ¡æ•°æ®è¢«å–å‡ºçš„æ¦‚ç‡æ˜¯ä¸€æ ·çš„, è€Œä¸æ˜¯ä»å‰é¢æ‹¿å‡º100æ¡æ¥.`

ä¸Šè¿°sampleå’Œrandomç¡®å®æ˜¯éšæœºçš„, ä½†æ˜¯ä¸èƒ½ä¿è¯è¾“å‡ºæ•°é‡çš„å¯æ§, åˆè¦éšæœº, åˆè¦æ¦‚ç‡ä¸€æ ·, è¾“å‡ºæ•°é‡å›ºå®š, é‚£ä¸å°±æ˜¯è“„æ°´æ± æŠ½æ ·äº†.

å®ç°èµ·æ¥ä¹Ÿå¾ˆç®€å•, æ€»é‡M, æŠ½å–N, è¢«é€‰ä¸­çš„æ¦‚ç‡æ˜¯N/M, é‚£ä¹ˆè·Ÿä¸Šé¢åˆ†åŒºcount/åˆ†åŒºtakeçš„å†™æ³•ç±»ä¼¼:

1. åˆ†åŒºcountç»Ÿè®¡æ¯ä¸ªåˆ†åŒºçš„æ€»é‡`xi`
2. é‚£ä¹ˆæ¯ä¸ªåˆ†åŒºåº”è¯¥æŠ½å–çš„æ•°é‡å°±æ˜¯`xi * N/M`
3. å†mapPartitionså¤„ç†, æ¯ä¸ªåˆ†åŒºå†…è¿›è¡Œè“„æ°´æ± æŠ½æ ·, å–`xi * N/M`æ¡.


{% highlight scala %}
   //  â‘   ç»Ÿè®¡æ¯ä¸ªåˆ†åŒºå†…è¡Œæ•°
    val x = df5_r.mapPartitions((a) => {
      val pid = TaskContext.getPartitionId()
      Iterator((pid, a.size))
    })
    val countByPart = x.collectAsList()
    print(countByPart) // [(0,379), (1,402), (2,605), (3,403), (4,211)]
   //  â‘¡  åˆ†é…å„åˆ†åŒºåº”è¯¥takeçš„æ•°é‡. xi * N/M
    val limit = 900
    val total = df5_r.count()
    val takeByPart = new Array[Int](countByPart.size)
    for (a <- 0 until countByPart.size) {
      val take = ((countByPart.get(a)._2 * limit) / total).toInt
      takeByPart(a) = take
    }
    print(takeByPart.sum)
    print(takeByPart.mkString("(", ", ", ")")) //(170, 180, 272, 181, 94)
    val takeByPartBC = spark.sparkContext.broadcast(takeByPart)
   //  â‘¢  åˆ†åŒºå†…è“„æ°´æ± æŠ½æ ·
    val result = df5_r.mapPartitions((stream) => {
      val pid = TaskContext.getPartitionId()
      val k = takeByPartBC.value(pid)

      val reservoir = new Array[Row](k)
      val random = new Random()
      var i = 0
      while (i < k && stream.hasNext) {
        reservoir(i) = stream.next()
        i += 1
      }
      while (stream.hasNext) {
        val j = random.nextInt(i + 1)
        if (j < k) reservoir(j) = stream.next()
        i += 1
      }

      reservoir.iterator
    })
    print(result.count())
{% endhighlight %}


## å‚è€ƒ
- sparkæºç , org/apache/spark/sql/execution/limit.scala
- æ–‡ä¸­è‹±æ–‡ç¿»è¯‘ç”±chatgptæä¾›
- <a href="/resources/sparklimit/Query 5 limitåshow.pdf">Query 5 limitåshowçš„æ‰§è¡Œæˆªå›¾</a>
- <a href="/resources/sparklimit/Query 6 limitåå†™å‡º.pdf">Query 6 limitåå†™å‡ºçš„æ‰§è¡Œæˆªå›¾</a>
- <a href="/resources/sparklimit/Query 9 limitåjoin.pdf">Query 9 limitåjoinçš„æ‰§è¡Œæˆªå›¾</a>
- <a href="/resources/sparklimit/Query 2 limitå‰sort.pdf">Query 2 limitå‰sortçš„æ‰§è¡Œæˆªå›¾</a>
